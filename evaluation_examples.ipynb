{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0a621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import string\n",
    "import re\n",
    "\n",
    "print(\"All libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d024749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
    "\n",
    "# --- CRITICAL: CONFIRM THESE PATHS ---\n",
    "# Make sure these paths point to the latest checkpoint folders for each model.\n",
    "LORA_MODEL_PATH = \"./backend/results_lora_final/checkpoint-36183\"  # Example path\n",
    "IA3_MODEL_PATH = \"./backend/results_ia3_final/checkpoint-36183\"   # <-- UPDATE THIS PATH\n",
    "# ---\n",
    "\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84c71d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# --- ALL HELPER FUNCTIONS ---\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features): features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "    predictions = collections.OrderedDict()\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        feature_indices = features_per_example[example_index]; min_null_score = None; valid_answers = []\n",
    "        context = example[\"context\"]\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]; end_logits = all_end_logits[feature_index]\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score: min_null_score = feature_null_score\n",
    "            start_indexes = np.argsort(start_logits)[-1:-n_best_size-1:-1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1:-n_best_size-1:-1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if (start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or\n",
    "                        offset_mapping[start_index] is None or offset_mapping[end_index] is None): continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length: continue\n",
    "                    start_char = offset_mapping[start_index][0]; end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append({\"score\": start_logits[start_index] + end_logits[end_index], \"text\": context[start_char:end_char]})\n",
    "        if len(valid_answers) > 0: best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else: best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return formatted_predictions, references\n",
    "\n",
    "def predict_custom(context, query, model, tokenizer):\n",
    "    model.eval()\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        query, \n",
    "        context, \n",
    "        return_tensors='pt', \n",
    "        max_length=512, \n",
    "        truncation=True\n",
    "    ).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "    answer_start = torch.argmax(start_logits)\n",
    "    answer_end = torch.argmax(end_logits) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "    if answer == tokenizer.cls_token: return \"[No answer found]\"\n",
    "    return answer\n",
    "\n",
    "def normalize_text(s):\n",
    "    def remove_articles(text): return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "    def white_space_fix(text): return \" \".join(text.split())\n",
    "    def remove_punc(text): return \"\".join(ch for ch in text if ch not in exclude)\n",
    "    exclude = set(string.punctuation)\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split(); truth_tokens = normalize_text(truth).split()\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0: return int(pred_tokens == truth_tokens)\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    if len(common_tokens) == 0: return 0\n",
    "    prec = len(common_tokens) / len(pred_tokens); rec = len(common_tokens) / len(truth_tokens)\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # This function is needed just to process the validation set for the predict() method\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"], examples[\"context\"], truncation=\"only_second\", max_length=256, stride=64,\n",
    "        return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\",\n",
    "    )\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "    return tokenized_examples\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6488d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation dataset...\n",
      "Validation data ready.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and the original SQuAD dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# We only need to process the validation set for evaluation\n",
    "print(\"Processing validation dataset...\")\n",
    "validation_features = dataset[\"validation\"].map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    "    desc=\"Tokenizing validation set\",\n",
    ")\n",
    "print(\"Validation data ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5df43703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned models from disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA and (IA)3 models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# This cell loads your trained models from the checkpoint files\n",
    "print(\"Loading fine-tuned models from disk...\")\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Apply the LoRA adapter\n",
    "lora_model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
    "lora_model = lora_model.to(DEVICE)\n",
    "\n",
    "# Apply the (IA)3 adapter (re-create base model to avoid conflicts)\n",
    "base_model_2 = AutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT)\n",
    "ia3_model = PeftModel.from_pretrained(base_model_2, IA3_MODEL_PATH)\n",
    "ia3_model = ia3_model.to(DEVICE)\n",
    "\n",
    "print(\"LoRA and (IA)3 models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64cc58a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Example Side-by-Side Analysis\n",
      "--- Example 1 ---\n",
      "Question: What is the name of the boundary that separated the formation of rocky planets from gas giants?\n",
      "True Answer: 'the frost line'\n",
      "Prediction from LoRA: 'the frost line'\n",
      "Prediction from (IA)³: 'mercury, venus, earth, and mars'\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: when did the solar system start\n",
      "True Answer: '4.6 billion years ago'\n",
      "Prediction from LoRA: '[No answer found]'\n",
      "Prediction from (IA)³: '4. 6 billion years ago with the gravitational collapse of a small part of a giant molecular cloud, often referred to as the solar nebula'\n",
      "\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: What was Guido van Rossum's primary goal when designing Python?\n",
      "True Answer: 'code readability'\n",
      "Prediction from LoRA: 'code readability'\n",
      "Prediction from (IA)³: '1991, python ' s design philosophy emphasizes code readability'\n",
      "\n",
      "\n",
      "--- Example 4 ---\n",
      "Question: Which conservatory at Gardens by the Bay holds a Guinness World Record?\n",
      "True Answer: 'the Flower Dome'\n",
      "Prediction from LoRA: 'the flower dome'\n",
      "Prediction from (IA)³: 'flower dome'\n",
      "\n",
      "\n",
      "--- Example 5 ---\n",
      "Question: What were the two colonial-era colleges that were merged to create the University of Malaya in 1949?\n",
      "True Answer: 'the King Edward VII College of Medicine and Raffles College'\n",
      "Prediction from LoRA: 'raffles college, was established in 1928 to promote education in arts and social sciences. on 8 october 1949, a significant milestone was reached when these two colleges were merged to create the university of malaya'\n",
      "Prediction from (IA)³: 'university of malaya'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Custom Example Side-by-Side Analysis\")\n",
    "\n",
    "custom_examples = [\n",
    "    {\"context\": \"The formation of our Solar System began approximately 4.6 billion years ago with the gravitational collapse of a small part of a giant molecular cloud, often referred to as the Solar Nebula. Most of the collapsing mass collected in the center, forming the Sun, while the rest flattened into a spinning, swirling protoplanetary disk out of which the planets, moons, asteroids, and other small Solar System bodies formed. This model, known as the nebular hypothesis, is the most widely accepted scientific model for the formation of star systems. As the disk spun, particles of dust and gas began to clump together through a process called accretion. Close to the young, hot protostar, only materials with high melting points, such as metals and silicates, could survive. This led to the formation of the rocky terrestrial planets: Mercury, Venus, Earth, and Mars. Farther out in the disk, beyond the asteroid belt, the environment was cooler. This temperature boundary, known as the frost line, was located between the orbits of present-day Mars and Jupiter. Beyond this line, volatile icy compounds were able to condense, allowing the gas giants Jupiter and Saturn, and the ice giants Uranus and Neptune, to grow to enormous sizes by accreting vast amounts of hydrogen and helium gas.\", \"question\": \"What is the name of the boundary that separated the formation of rocky planets from gas giants?\", \"answer\": \"the frost line\"},\n",
    "    {\"context\": \"The formation of our Solar System began approximately 4.6 billion years ago with the gravitational collapse of a small part of a giant molecular cloud, often referred to as the Solar Nebula. Most of the collapsing mass collected in the center, forming the Sun, while the rest flattened into a spinning, swirling protoplanetary disk out of which the planets, moons, asteroids, and other small Solar System bodies formed. This model, known as the nebular hypothesis, is the most widely accepted scientific model for the formation of star systems. As the disk spun, particles of dust and gas began to clump together through a process called accretion. Close to the young, hot protostar, only materials with high melting points, such as metals and silicates, could survive. This led to the formation of the rocky terrestrial planets: Mercury, Venus, Earth, and Mars. Farther out in the disk, beyond the asteroid belt, the environment was cooler. This temperature boundary, known as the frost line, was located between the orbits of present-day Mars and Jupiter. Beyond this line, volatile icy compounds were able to condense, allowing the gas giants Jupiter and Saturn, and the ice giants Uranus and Neptune, to grow to enormous sizes by accreting vast amounts of hydrogen and helium gas.\", \"question\": \"when did the solar system start\", \"answer\": \"4.6 billion years ago\"},\n",
    "    {\"context\": \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant indentation. This core philosophy is often summarized in the Zen of Python, which includes aphorisms like `Beautiful is better than ugly` and `Simple is better than complex.` The language's name is a tribute to the British comedy group Monty Python, as Rossum was a fan of their show, Monty Python's Flying Circus.\", \"question\": \"What was Guido van Rossum's primary goal when designing Python?\", \"answer\": \"code readability\"},\n",
    "    {\"context\": \"Gardens by the Bay is a nature park spanning 101 hectares in the Central Region of Singapore, adjacent to the Marina Reservoir. The park consists of three waterfront gardens and is famous for its iconic Supertree Grove. It also features two large cooled conservatories for visitors. Inside the Cloud Forest conservatory, guests can experience a cool-moist climate and see a 35-metre-tall indoor waterfall, which is the world's second tallest. The other conservatory, the Flower Dome, holds the Guinness World Record as the world's largest glass greenhouse.\", \"question\": \"Which conservatory at Gardens by the Bay holds a Guinness World Record?\", \"answer\": \"the Flower Dome\"},\n",
    "    {\"context\": \"The National University of Singapore (NUS) is the national research university of Singapore. Founded in 1905 as the Straits Settlements and Federated Malay States Government Medical School, it is the oldest autonomous university in the country. The institution's history is complex, reflecting the development of Singapore as a nation. The medical school was renamed the King Edward VII College of Medicine in 1921. A second institution, Raffles College, was established in 1928 to promote education in arts and social sciences. On 8 October 1949, a significant milestone was reached when these two colleges were merged to create the University of Malaya. Following the decolonization of Malaya, the university was split into two autonomous divisions in 1959: one in Kuala Lumpur and one in Singapore. The Singapore division, located in Bukit Timah, was later established as the independent University of Singapore in 1962. The modern NUS was finally formed in 1980 through a merger between the University of Singapore and Nanyang University.\", \"question\": \"What were the two colonial-era colleges that were merged to create the University of Malaya in 1949?\", \"answer\": \"the King Edward VII College of Medicine and Raffles College\"},\n",
    "]\n",
    "\n",
    "# A single loop to get predictions from both models for each question\n",
    "for i, example in enumerate(custom_examples):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    true_answer = example[\"answer\"]\n",
    "\n",
    "    # Get prediction from both models\n",
    "    lora_prediction = predict_custom(context, question, lora_model, tokenizer)\n",
    "    ia3_prediction = predict_custom(context, question, ia3_model, tokenizer)\n",
    "    \n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"True Answer: '{true_answer}'\")\n",
    "    print(f\"Prediction from LoRA: '{lora_prediction}'\")\n",
    "    print(f\"Prediction from (IA)³: '{ia3_prediction}'\")\n",
    "    print(\"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
